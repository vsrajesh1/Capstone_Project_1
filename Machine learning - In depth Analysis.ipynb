{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        business_id   anzsic_code         metro      sv_month       sv_year  \\\n",
      "count  53646.000000  53596.000000  53646.000000  11147.000000  11147.000000   \n",
      "mean   26827.169258   7423.195780      0.058308      6.606262   2012.755898   \n",
      "std    15488.324811   2428.965571      0.234327      3.435323      1.818195   \n",
      "min        1.000000    100.000000      0.000000      1.000000   2009.000000   \n",
      "25%    13414.250000   5259.000000      0.000000      4.000000   2011.000000   \n",
      "50%    26827.500000   7869.000000      0.000000      6.000000   2012.000000   \n",
      "75%    40239.750000   9525.000000      0.000000     10.000000   2014.000000   \n",
      "max    53654.000000   9901.000000      1.000000     12.000000   2017.000000   \n",
      "\n",
      "       sv_hours_work  sv_staff_lt35h  sv_staff_gt35h  sv_end_train  \\\n",
      "count    8102.000000     5803.000000     5296.000000   3049.000000   \n",
      "mean       35.280449        0.558849        0.405778      1.296491   \n",
      "std        21.909007        2.007426        2.342488      0.598547   \n",
      "min         0.000000        0.000000        0.000000      1.000000   \n",
      "25%        20.000000        0.000000        0.000000      1.000000   \n",
      "50%        35.000000        0.000000        0.000000      1.000000   \n",
      "75%        45.000000        1.000000        0.000000      1.000000   \n",
      "max       120.000000       99.000000       55.000000      3.000000   \n",
      "\n",
      "       sv_end_mentor  sv_end_profit   sv_end_dem   sv_end_loc  sv_end_health  \\\n",
      "count    3075.000000    3227.000000  3144.000000  2969.000000    3044.000000   \n",
      "mean        1.386992       2.335606     1.986323     1.458404       1.499671   \n",
      "std         0.684782       0.821582     0.823689     0.712119       0.781607   \n",
      "min         1.000000       1.000000     1.000000     1.000000       1.000000   \n",
      "25%         1.000000       2.000000     1.000000     1.000000       1.000000   \n",
      "50%         1.000000       3.000000     2.000000     1.000000       1.000000   \n",
      "75%         2.000000       3.000000     3.000000     2.000000       2.000000   \n",
      "max         3.000000       3.000000     3.000000     3.000000       3.000000   \n",
      "\n",
      "        sv_end_oth  sv_sat_bus_train  sv_sat_mentor  sv_tailor_sup  \\\n",
      "count  2699.000000      10700.000000   10593.000000   10582.000000   \n",
      "mean      2.086328          4.081682       3.864628       3.716689   \n",
      "std       0.923077          1.177991       1.325153       1.404108   \n",
      "min       1.000000          1.000000       1.000000       1.000000   \n",
      "25%       1.000000          4.000000       4.000000       3.000000   \n",
      "50%       2.000000          4.000000       4.000000       4.000000   \n",
      "75%       3.000000          5.000000       5.000000       5.000000   \n",
      "max       3.000000          5.000000       5.000000       5.000000   \n",
      "\n",
      "       sv_sat_overall  \n",
      "count    10408.000000  \n",
      "mean         3.877786  \n",
      "std          1.305337  \n",
      "min          1.000000  \n",
      "25%          4.000000  \n",
      "50%          4.000000  \n",
      "75%          5.000000  \n",
      "max          5.000000  \n"
     ]
    }
   ],
   "source": [
    "#Prepare the data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Retrieve the data from CSV files and populate it in a data frame.\n",
    "dictionary = pd.read_excel('Data/neis-data-dictionary_terms.xlsx')\n",
    "main       = pd.read_excel(\"Data/neisdatagovhack_Main.xlsx\")\n",
    "dict_df = pd.DataFrame.from_dict(dictionary, orient='columns', dtype=None)\n",
    "main_df = pd.DataFrame.from_dict(main, orient='columns', dtype=None)\n",
    "print(main_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>anzsic_code</th>\n",
       "      <th>metro</th>\n",
       "      <th>sv_month</th>\n",
       "      <th>sv_year</th>\n",
       "      <th>sv_hours_work</th>\n",
       "      <th>sv_staff_lt35h</th>\n",
       "      <th>sv_staff_gt35h</th>\n",
       "      <th>sv_end_train</th>\n",
       "      <th>sv_end_mentor</th>\n",
       "      <th>sv_end_profit</th>\n",
       "      <th>sv_end_dem</th>\n",
       "      <th>sv_end_loc</th>\n",
       "      <th>sv_end_health</th>\n",
       "      <th>sv_end_oth</th>\n",
       "      <th>sv_sat_bus_train</th>\n",
       "      <th>sv_sat_mentor</th>\n",
       "      <th>sv_tailor_sup</th>\n",
       "      <th>sv_sat_overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>53646.000000</td>\n",
       "      <td>53596.000000</td>\n",
       "      <td>53646.000000</td>\n",
       "      <td>11147.000000</td>\n",
       "      <td>11147.000000</td>\n",
       "      <td>53646.000000</td>\n",
       "      <td>5803.000000</td>\n",
       "      <td>5296.000000</td>\n",
       "      <td>3049.000000</td>\n",
       "      <td>3075.000000</td>\n",
       "      <td>3227.000000</td>\n",
       "      <td>3144.000000</td>\n",
       "      <td>2969.000000</td>\n",
       "      <td>3044.000000</td>\n",
       "      <td>2699.000000</td>\n",
       "      <td>10700.000000</td>\n",
       "      <td>10593.000000</td>\n",
       "      <td>10582.000000</td>\n",
       "      <td>10408.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>26827.169258</td>\n",
       "      <td>7423.195780</td>\n",
       "      <td>0.058308</td>\n",
       "      <td>6.606262</td>\n",
       "      <td>2012.755898</td>\n",
       "      <td>35.280449</td>\n",
       "      <td>0.558849</td>\n",
       "      <td>0.405778</td>\n",
       "      <td>1.296491</td>\n",
       "      <td>1.386992</td>\n",
       "      <td>2.335606</td>\n",
       "      <td>1.986323</td>\n",
       "      <td>1.458404</td>\n",
       "      <td>1.499671</td>\n",
       "      <td>2.086328</td>\n",
       "      <td>4.081682</td>\n",
       "      <td>3.864628</td>\n",
       "      <td>3.716689</td>\n",
       "      <td>3.877786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15488.324811</td>\n",
       "      <td>2428.965571</td>\n",
       "      <td>0.234327</td>\n",
       "      <td>3.435323</td>\n",
       "      <td>1.818195</td>\n",
       "      <td>8.513877</td>\n",
       "      <td>2.007426</td>\n",
       "      <td>2.342488</td>\n",
       "      <td>0.598547</td>\n",
       "      <td>0.684782</td>\n",
       "      <td>0.821582</td>\n",
       "      <td>0.823689</td>\n",
       "      <td>0.712119</td>\n",
       "      <td>0.781607</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.177991</td>\n",
       "      <td>1.325153</td>\n",
       "      <td>1.404108</td>\n",
       "      <td>1.305337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13414.250000</td>\n",
       "      <td>5259.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2011.000000</td>\n",
       "      <td>35.280449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>26827.500000</td>\n",
       "      <td>7869.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>35.280449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>40239.750000</td>\n",
       "      <td>9525.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>35.280449</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>53654.000000</td>\n",
       "      <td>9901.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        business_id   anzsic_code         metro      sv_month       sv_year  \\\n",
       "count  53646.000000  53596.000000  53646.000000  11147.000000  11147.000000   \n",
       "mean   26827.169258   7423.195780      0.058308      6.606262   2012.755898   \n",
       "std    15488.324811   2428.965571      0.234327      3.435323      1.818195   \n",
       "min        1.000000    100.000000      0.000000      1.000000   2009.000000   \n",
       "25%    13414.250000   5259.000000      0.000000      4.000000   2011.000000   \n",
       "50%    26827.500000   7869.000000      0.000000      6.000000   2012.000000   \n",
       "75%    40239.750000   9525.000000      0.000000     10.000000   2014.000000   \n",
       "max    53654.000000   9901.000000      1.000000     12.000000   2017.000000   \n",
       "\n",
       "       sv_hours_work  sv_staff_lt35h  sv_staff_gt35h  sv_end_train  \\\n",
       "count   53646.000000     5803.000000     5296.000000   3049.000000   \n",
       "mean       35.280449        0.558849        0.405778      1.296491   \n",
       "std         8.513877        2.007426        2.342488      0.598547   \n",
       "min         0.000000        0.000000        0.000000      1.000000   \n",
       "25%        35.280449        0.000000        0.000000      1.000000   \n",
       "50%        35.280449        0.000000        0.000000      1.000000   \n",
       "75%        35.280449        1.000000        0.000000      1.000000   \n",
       "max       120.000000       99.000000       55.000000      3.000000   \n",
       "\n",
       "       sv_end_mentor  sv_end_profit   sv_end_dem   sv_end_loc  sv_end_health  \\\n",
       "count    3075.000000    3227.000000  3144.000000  2969.000000    3044.000000   \n",
       "mean        1.386992       2.335606     1.986323     1.458404       1.499671   \n",
       "std         0.684782       0.821582     0.823689     0.712119       0.781607   \n",
       "min         1.000000       1.000000     1.000000     1.000000       1.000000   \n",
       "25%         1.000000       2.000000     1.000000     1.000000       1.000000   \n",
       "50%         1.000000       3.000000     2.000000     1.000000       1.000000   \n",
       "75%         2.000000       3.000000     3.000000     2.000000       2.000000   \n",
       "max         3.000000       3.000000     3.000000     3.000000       3.000000   \n",
       "\n",
       "        sv_end_oth  sv_sat_bus_train  sv_sat_mentor  sv_tailor_sup  \\\n",
       "count  2699.000000      10700.000000   10593.000000   10582.000000   \n",
       "mean      2.086328          4.081682       3.864628       3.716689   \n",
       "std       0.923077          1.177991       1.325153       1.404108   \n",
       "min       1.000000          1.000000       1.000000       1.000000   \n",
       "25%       1.000000          4.000000       4.000000       3.000000   \n",
       "50%       2.000000          4.000000       4.000000       4.000000   \n",
       "75%       3.000000          5.000000       5.000000       5.000000   \n",
       "max       3.000000          5.000000       5.000000       5.000000   \n",
       "\n",
       "       sv_sat_overall  \n",
       "count    10408.000000  \n",
       "mean         3.877786  \n",
       "std          1.305337  \n",
       "min          1.000000  \n",
       "25%          4.000000  \n",
       "50%          4.000000  \n",
       "75%          5.000000  \n",
       "max          5.000000  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Cleaning - This step is already performed as part of Data wrangling. Features are identified.\n",
    "# Replace Nan/ missing values in the feature 'SV_HOURS_WORK' with the mean value of the non-NAN values.\n",
    "main_df.sv_hours_work = main_df.sv_hours_work.fillna(main_df.sv_hours_work.mean())\n",
    "main_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>metro</th>\n",
       "      <th>gender_cd</th>\n",
       "      <th>sv_hours_work</th>\n",
       "      <th>indigenous_ind</th>\n",
       "      <th>ex_offender_ind</th>\n",
       "      <th>nesb_ind</th>\n",
       "      <th>refugee_ind</th>\n",
       "      <th>disability_ind</th>\n",
       "      <th>homeless_ind</th>\n",
       "      <th>sole_parent_ind</th>\n",
       "      <th>success_indicator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>53646.000000</td>\n",
       "      <td>53646.000000</td>\n",
       "      <td>53646.000000</td>\n",
       "      <td>53646.000000</td>\n",
       "      <td>53646.000000</td>\n",
       "      <td>53646.000000</td>\n",
       "      <td>53646.000000</td>\n",
       "      <td>53646.000000</td>\n",
       "      <td>53646.000000</td>\n",
       "      <td>53646.000000</td>\n",
       "      <td>53646.000000</td>\n",
       "      <td>53646.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>26827.169258</td>\n",
       "      <td>0.058308</td>\n",
       "      <td>0.516143</td>\n",
       "      <td>35.280449</td>\n",
       "      <td>0.475171</td>\n",
       "      <td>0.464415</td>\n",
       "      <td>0.449185</td>\n",
       "      <td>0.347854</td>\n",
       "      <td>0.339653</td>\n",
       "      <td>0.138184</td>\n",
       "      <td>0.100380</td>\n",
       "      <td>0.685046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15488.324811</td>\n",
       "      <td>0.234327</td>\n",
       "      <td>0.499744</td>\n",
       "      <td>8.513877</td>\n",
       "      <td>0.499388</td>\n",
       "      <td>0.498737</td>\n",
       "      <td>0.497416</td>\n",
       "      <td>0.476294</td>\n",
       "      <td>0.473596</td>\n",
       "      <td>0.345096</td>\n",
       "      <td>0.300509</td>\n",
       "      <td>0.464502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13414.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.280449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>26827.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.280449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>40239.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.280449</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>53654.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        business_id         metro     gender_cd  sv_hours_work  \\\n",
       "count  53646.000000  53646.000000  53646.000000   53646.000000   \n",
       "mean   26827.169258      0.058308      0.516143      35.280449   \n",
       "std    15488.324811      0.234327      0.499744       8.513877   \n",
       "min        1.000000      0.000000      0.000000       0.000000   \n",
       "25%    13414.250000      0.000000      0.000000      35.280449   \n",
       "50%    26827.500000      0.000000      1.000000      35.280449   \n",
       "75%    40239.750000      0.000000      1.000000      35.280449   \n",
       "max    53654.000000      1.000000      1.000000     120.000000   \n",
       "\n",
       "       indigenous_ind  ex_offender_ind      nesb_ind   refugee_ind  \\\n",
       "count    53646.000000     53646.000000  53646.000000  53646.000000   \n",
       "mean         0.475171         0.464415      0.449185      0.347854   \n",
       "std          0.499388         0.498737      0.497416      0.476294   \n",
       "min          0.000000         0.000000      0.000000      0.000000   \n",
       "25%          0.000000         0.000000      0.000000      0.000000   \n",
       "50%          0.000000         0.000000      0.000000      0.000000   \n",
       "75%          1.000000         1.000000      1.000000      1.000000   \n",
       "max          1.000000         1.000000      1.000000      1.000000   \n",
       "\n",
       "       disability_ind  homeless_ind  sole_parent_ind  success_indicator  \n",
       "count    53646.000000  53646.000000     53646.000000       53646.000000  \n",
       "mean         0.339653      0.138184         0.100380           0.685046  \n",
       "std          0.473596      0.345096         0.300509           0.464502  \n",
       "min          0.000000      0.000000         0.000000           0.000000  \n",
       "25%          0.000000      0.000000         0.000000           0.000000  \n",
       "50%          0.000000      0.000000         0.000000           1.000000  \n",
       "75%          1.000000      0.000000         0.000000           1.000000  \n",
       "max          1.000000      1.000000         1.000000           1.000000  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Representing the gender as NUMBERS i.e. Male: 1 and Female: 0\n",
    "def convert_gender(x):\n",
    "    if \"M\" in x:\n",
    "        return 1\n",
    "    elif \"F\" in x:\n",
    "        return 0\n",
    "    else: return 0\n",
    "    \n",
    "# Representing the Success_indicators as NUMBERS i.e. SUCCESS - Y: 1 and SUCCESS - N (Failure): 0    \n",
    "def convert_successful(x):\n",
    "    if \"Y\" in x:\n",
    "        return 1\n",
    "    elif \"N\" in x:\n",
    "        return 0\n",
    "    else: return 0   \n",
    "# Bring all the personality types as a single column in the Data frame.\n",
    "def identify_personality(x):\n",
    "    if x['indigenous_ind'] == 'Y':\n",
    "        #return \"indigenous_ind\"\n",
    "        return 1        \n",
    "    elif x['ex_offender_ind'] == 'Y':\n",
    "        #return \"ex_offender_ind\"\n",
    "        return 1        \n",
    "    elif x['nesb_ind'] == 'Y':\n",
    "        #return \"nesb_ind\"\n",
    "        return 1        \n",
    "    elif x['refugee_ind'] == 'Y':\n",
    "        #return \"refugee_ind\"\n",
    "        return 1        \n",
    "    elif x['disability_ind'] == 'Y':\n",
    "        #return \"disability_ind\"\n",
    "        return 1        \n",
    "    elif x['homeless_ind'] == 'Y':\n",
    "        #return \"homeless_ind\"\n",
    "        return 1        \n",
    "    elif x['sole_parent_ind'] == 'Y':\n",
    "        #return \"sole_parent_ind\"\n",
    "        return 1\n",
    "    else:\n",
    "        #return \"NO_PERSONALITY\"    \n",
    "        return 0\n",
    "\n",
    "# Some of the columns store text/ Business IDs and they dont make much sense. Therefore, consider numerical columns.\n",
    "main_model = main_df[['business_id','exit_reason','state','metro','age_group','gender_cd','industry_type',\n",
    "                      'sv_hours_work','indigenous_ind','ex_offender_ind','nesb_ind','refugee_ind','disability_ind','homeless_ind',\n",
    "                      'sole_parent_ind','successful']].copy()\n",
    "#we need to find the \"Success percentage per community/personality type\" and check whether this correlates\n",
    "# with our target.\n",
    "#main_model['personality_type'] = main_model.apply(lambda row: identify_personality(row),axis=1)\n",
    "main_model['indigenous_ind'] = main_model.apply(lambda row: identify_personality(row),axis=1)\n",
    "main_model.ex_offender_ind   = main_model.apply(lambda row: identify_personality(row),axis=1)\n",
    "main_model.nesb_ind          = main_model.apply(lambda row: identify_personality(row),axis=1)\n",
    "main_model.refugee_ind       = main_model.apply(lambda row: identify_personality(row),axis=1)\n",
    "main_model.disability_ind    = main_model.apply(lambda row: identify_personality(row),axis=1)\n",
    "main_model.homeless_ind      = main_model.apply(lambda row: identify_personality(row),axis=1)\n",
    "main_model.sole_parent_ind   = main_model.apply(lambda row: identify_personality(row),axis=1)\n",
    "\n",
    "# Some of the features in the above dataframe above text/ descriptions and these can be converted to numerical equivalents.\n",
    "# for Instance, in \"gender_cd\", male can be represented as 1 and female can be represented as 0   \n",
    "main_model[\"success_indicator\"] = main_model[\"successful\"].apply(convert_successful)\n",
    "#main_model[\"neis_allowance_ind\"] = main_model[\"neis_allowance_ind\"].apply(convert_gender) # function for \"gender\" can be applied here\n",
    "main_model[\"gender_cd\"] = main_model[\"gender_cd\"].apply(convert_gender)\n",
    "\n",
    "main_model.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segregating the Features and the TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53646,)\n",
      "(53646, 45)\n",
      "[ True  True False ...  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "# Before building the features, create dummy variables for the categorical variables.\n",
    "X = main_model[['industry_type','state','gender_cd','sv_hours_work','metro','age_group','indigenous_ind','ex_offender_ind','nesb_ind','refugee_ind','disability_ind','homeless_ind',\n",
    "                      'sole_parent_ind']]\n",
    "y = (main_model.success_indicator == 1).values # main_model['success_indicator']\n",
    "\n",
    "n_industry = pd.get_dummies(main_model.industry_type)\n",
    "X = pd.concat([X, n_industry], axis=1)\n",
    "\n",
    "n_state = pd.get_dummies(main_model.state)\n",
    "X = pd.concat([X, n_state], axis=1)\n",
    "\n",
    "#n_metro = pd.get_dummies(main_model.metro)\n",
    "#X = pd.concat([X, n_metro], axis=1)\n",
    "\n",
    "n_age = pd.get_dummies(main_model.age_group)\n",
    "X = pd.concat([X, n_age], axis=1)\n",
    "\n",
    "\n",
    "drop_col = ['industry_type', 'state','age_group']\n",
    "X.drop(drop_col, inplace=True, axis=1)\n",
    "X = X.values\n",
    "print(y.shape)\n",
    "print(X.shape)\n",
    "\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53646, 45) (53646,)\n",
      "\n",
      "\n",
      "[Test] Accuracy score (y_pred_test, y_test): 0.6927378467044438\n",
      "\n",
      "\n",
      "[Train] Accuracy score (y_pred_train, y_train): 0.6957051250186409\n",
      "Classification Report for Training data: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.59      0.10      0.17     12632\n",
      "       True       0.70      0.97      0.81     27602\n",
      "\n",
      "avg / total       0.67      0.70      0.61     40234\n",
      "\n",
      "\n",
      "\n",
      "Classification Report for Test data: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.60      0.10      0.18      4264\n",
      "       True       0.70      0.97      0.81      9148\n",
      "\n",
      "avg / total       0.67      0.69      0.61     13412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now the data is ready. Build a logistic regression model and apply the training data to it.\n",
    "# Import the necessary modules\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "print(X.shape,y.shape)\n",
    "# infinite values and NAN are causing issues while attempting to fit the data. \n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25, random_state=5)\n",
    "\n",
    "# Create the classifier: logreg\n",
    "logreg = LogisticRegression() # penalty='l1'\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred_test = logreg.predict(X_test)\n",
    "print(\"\\n\")\n",
    "print(\"[Test] Accuracy score (y_pred_test, y_test):\",accuracy_score(y_pred_test, y_test))\n",
    "\n",
    "y_pred_train = logreg.predict(X_train)\n",
    "print(\"\\n\")\n",
    "print(\"[Train] Accuracy score (y_pred_train, y_train):\",accuracy_score(y_pred_train, y_train))\n",
    "\n",
    "\n",
    "print(\"Classification Report for Training data: \")\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "\n",
    "# Compute and print the classification report\n",
    "print(\"\\n\")\n",
    "print('Classification Report for Test data: ')\n",
    "print(classification_report(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above results:\n",
    "\n",
    "    1) Both training and test accuracy scores are almost the same and so there is no OVERFITTING\n",
    "    \n",
    "    2) There is no large gap between any of the observed metrics - PRECISION, RECALL & F1-SCORE.\n",
    "    \n",
    "    3) We find that there are more successfull businesses than failures (False/True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HyperParameter Tuning - Determine Reg.parameter 'C' using Grid search with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find the Score and Classification for Training data:\n",
      "\n",
      "\n",
      "0.6856390808153231\n",
      "\n",
      "\n",
      "0.695506352947994\n",
      "\n",
      "\n",
      "0.6954069308386186\n",
      "\n",
      "\n",
      "0.6953820706777956\n",
      "\n",
      "\n",
      "0.695431778643497\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "def cv_score(clf, x, y, score_func=accuracy_score):\n",
    "    result = 0\n",
    "    nfold = 5\n",
    "    for train, test in KFold(nfold).split(x): # split data into train/test groups, 5 times\n",
    "        clf.fit(x[train], y[train]) # fit\n",
    "        result += score_func(clf.predict(x[test]), y[test]) # evaluate score function on held-out data         \n",
    "\n",
    "    #y_pred_train = clf.predict(x)\n",
    "    #classification = classification_report(y, y_pred_train)\n",
    "    return result / nfold #,classification # average\n",
    "\n",
    "#the grid of parameters to search over\n",
    "Cs = [0.001, 0.1, 1, 10, 100]\n",
    "\n",
    "print('Find the Score and Classification for Training data:')\n",
    "print('\\n')\n",
    "#from sklearn.linear_model import LogisticRegressionCV\n",
    "for i in Cs: \n",
    "    clf_1 = LogisticRegression(C=i)\n",
    "    score_train = cv_score(clf_1, X_train, y_train)    # classification_train\n",
    "    print(score_train)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization parameter C = 0.1 appears to have yilded the highest value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing Grid Search based on CV = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'C': 0.1}\n",
      "Best score is 0.6955062882139484\n",
      "Accuracy score for Grid search: 0.6930360870861915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Setup the hyperparameter grid\n",
    "param_grid = {'C': [0.1]}\n",
    "\n",
    "# Instantiate a logistic regression classifier: logreg\n",
    "logreg = LogisticRegression() ## Not introducing penalty L1/ L2 as there is no overfitting\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "logreg_cv.fit(X_train,y_train)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))\n",
    "\n",
    "result = accuracy_score(logreg_cv.predict(X_test), y_test)\n",
    "print('Accuracy score for Grid search:',result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification reports based on GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Train] Accuracy score (y_pred_train, y_train): 0.6960282348262663\n",
      "Classification Report for Training data: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.60      0.10      0.17     12632\n",
      "       True       0.70      0.97      0.81     27602\n",
      "\n",
      "avg / total       0.67      0.70      0.61     40234\n",
      "\n",
      "\n",
      "\n",
      "[Test] Accuracy score (y_pred_test, y_test): 0.6930360870861915\n",
      "Classification Report for Test data: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.60      0.10      0.17      4264\n",
      "       True       0.70      0.97      0.81      9148\n",
      "\n",
      "avg / total       0.67      0.69      0.61     13412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predcv_train = logreg_cv.predict(X_train)\n",
    "print(\"\\n\")\n",
    "print(\"[Train] Accuracy score (y_pred_train, y_train):\",accuracy_score(y_predcv_train, y_train))\n",
    "\n",
    "print(\"Classification Report for Training data: \")\n",
    "print(classification_report(y_train, y_predcv_train))\n",
    "\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_predcv_test = logreg_cv.predict(X_test)\n",
    "print(\"\\n\")\n",
    "print(\"[Test] Accuracy score (y_pred_test, y_test):\",accuracy_score(y_predcv_test, y_test))\n",
    "\n",
    "\n",
    "# Compute and print the classification report\n",
    "print('Classification Report for Test data: ')\n",
    "print(classification_report(y_test, y_predcv_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion\n",
    "\n",
    "Based on the above results:\n",
    "    \n",
    "    1) We observed no major differences between the training and test data even after performing GridSearch based on \n",
    "    Regularization parameter 'C'\n",
    "    2) There is no overfitting of data and so the penalties L1 and L2 were not used during GridSearch_CV."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
